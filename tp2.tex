\documentclass[12pt,a4paper]{report}

% --------------------------------------------------------
% PAQUETES BÁSICOS
% --------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{listings}   % Para mostrar código fuente
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{fancyhdr}

% --------------------------------------------------------
% CONFIGURACIÓN DE LISTINGS (código Python)
% --------------------------------------------------------
\lstset{
    language=Python,
    backgroundcolor=\color{gray!10},
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    stringstyle=\color{red!60!black},
    commentstyle=\color{green!50!black}\itshape,
    showstringspaces=false,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray},
    breaklines=true,
    captionpos=b
}

% --------------------------------------------------------
% CONFIGURACIONES GENERALES
% --------------------------------------------------------
\geometry{margin=2.5cm}
\setstretch{1.3}
\pagestyle{fancy}
\fancyhf{}
\lhead{Nombre del Estudiante}
\rhead{Título del Trabajo}
\cfoot{\thepage}
\setlength{\headheight}{15pt} % <-- Soluciona el error

% --------------------------------------------------------
% DATOS DE PORTADA
% --------------------------------------------------------
\title{
    \vspace{2cm}
    \textbf{Título del Reporte}\\[1cm]
    \large Nombre del Curso\\[0.3cm]
    Profesor: Nombre del Profesor\\[0.3cm]
    Universidad o Institución\\[2cm]
}
\author{Nombre del Estudiante}
\date{\today}

% --------------------------------------------------------
% DOCUMENTO
% --------------------------------------------------------
\begin{document}

\maketitle
\thispagestyle{empty}
\newpage

% --------------------------------------------------------
% ÍNDICE
% --------------------------------------------------------
\tableofcontents
\newpage

% --------------------------------------------------------
% SECCIONES DEL INFORME
% --------------------------------------------------------

\section{Distancia de Bhattacharyya}

Se utilizó el código mostrado en \autoref{lst:bhattacharyya} para comparar las funciones de densidad estimadas con el ajuste del modelo mixto Gaussiano de Kittler, $p(x)$, y la aproximación de la densidad mediante el histograma de los datos, $q(x)$. El valor obtenido, mostrado en \autoref{tab:bhattacharyya_result}, indica que las densidades $p(x)$ y $q(x)$ son estadísticamente muy similares, lo que sugiere que el modelo de Kittler se ajusta de manera muy precisa a la distribución real de los datos.


\begin{table}[!h]
\centering
\caption{Distancia de Bhattacharyya}
\label{tab:bhattacharyya_result}
\begin{tabular}{lc}
\hline
\textbf{Métrica} & \textbf{Valor} \\
\hline
Distancia de Bhattacharyya & $2.4031 \times 10^{-5}$ \\
\hline
\end{tabular}
\end{table}


\begin{lstlisting}[caption={Función en Python de la distancia de Bhattacharyya}, label={lst:bhattacharyya}]
def calcular_bhattacharyya_distance(p: torch.Tensor, q: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:
    """
    Calcula la distancia de Bhattacharyya entre dos funciones de densidad de probabilidad.

    Parametros
    ----------
    p : torch.Tensor
        Tensor 1D que representa la primera funcion de densidad de probabilidad (PDF).
    q : torch.Tensor
        Tensor 1D que representa la segunda PDF.
    eps : float
        Valor diminuto para evitar log(0) o divisiones por cero.

    Retorna
    -------
    torch.Tensor
        Escalar con la distancia de Bhattacharyya.
    """
    p = p / (p.sum() + eps)
    q = q / (q.sum() + eps)

    bc = torch.sum(torch.sqrt(p * q))
    bc = torch.clamp(bc, min=eps, max=1.0)
    distance = -torch.log(bc)
    return distance.item()
\end{lstlisting}

\subsection{Relación entre Bhattacharyya y estimación de parámetros de Kittler}
La distancia de Bhattacharyya mide el grado en el que dos distribuciones de probabilidad se solapan. En el contexto del algoritmo de Kittler, una de las distribuciones corresponde al modelo estimado mediante la mezcla gaussiana, $p(x)$, y la otra al histograma real de los datos, $q(x)$. 

El proceso de estimación de parámetros en el algoritmo de Kittler busca ajustar los parámetros óptimos de las gaussianas (medias, varianzas y pesos) de manera que el modelo mixto describa de la mejor forma posible la distribución real. 

Cuando la distancia de Bhattacharyya \textbf{disminuye}, el solapamiento entre $p(x)$ y $q(x)$ aumenta, indicando que el modelo estimado se aproxima más fielmente a la distribución real y que los parámetros obtenidos son adecuados. Por el contrario, cuando la distancia de Bhattacharyya \textbf{aumenta}, las distribuciones se solapan menos, lo que refleja un menor ajuste del modelo mixto a la distribución real y, por ende, una estimación menos precisa de los parámetros y del umbral óptimo.

\section{Punto 3}
\subsection{Método de mejora de umbralización}

El método de umbralización de Kittler, parte de la idea de que los niveles de gris de una imagen pueden modelarse como una mezcla de dos distribuciones gaussianas: una que representa el fondo y otra que representa el objeto. El algoritmo busca el umbral $T$ que minimiza la probabilidad de clasificar erróneamente un píxel, lo cual se logra calculando, para cada valor posible de $T$, los parámetros (media, varianza y probabilidad a priori) de ambas clases a partir del histograma de la imagen.

Sin embargo, este procedimiento tiene una limitación. Cuando se evalúa un umbral $T$, las estimaciones de los parámetros de cada clase se realizan sobre las partes del histograma separadas por dicho umbral: los valores menores o iguales a $T$ para el fondo, y los mayores a $T$ para el objeto. Estas porciones no representan las distribuciones gaussianas completas, sino \textbf{distribuciones truncadas}, ya que las colas de las campanas quedan fuera del rango utilizado. Al estimar directamente la media y la varianza sobre estas regiones truncadas, se introduce un \textbf{sesgo}: las medias tienden a desplazarse hacia el umbral y las varianzas resultan subestimadas. Este sesgo afecta negativamente el criterio de error de Kittler y puede llevar a seleccionar un umbral subóptimo, especialmente cuando las distribuciones de fondo y objeto se superponen significativamente.

Para corregir este problema, \textbf{Cho, et al (1989)} propusieron una mejora al método original. Su propuesta consiste en \textbf{compensar el sesgo introducido por el truncamiento}. En lugar de asumir que los datos a cada lado del umbral provienen de una distribución normal completa, los autores asumen que provienen de una normal truncada y, a partir de ella, calcula el valor esperado y la varianza teóricos, y con ello estimar los parámetros de la distribución original no truncada.

Formalmente, si $X \sim \mathcal{N}(\mu, \sigma^2)$ es una variable normal truncada en su cola izquierda en $T$ (\(X < T\)), la media y varianza del truncamiento se expresan como:

\[
\mu = \mu_t + \sigma \frac{\phi(z)}{\Phi(z)}, \qquad
\sigma^2 = \frac{\sigma_t^2}{1 - \frac{\phi(z)}{\Phi(z)} \left( z + \frac{\phi(z)}{\Phi(z)} \right)}
\]

donde \(z = \frac{T - \mu}{\sigma}\), \(\phi(z)\) es la función de densidad de la normal estándar y \(\Phi(z)\) su función de distribución acumulada. De manera análoga, para el caso de truncamiento inferior (cuando \(X < T\)), se obtienen:

\[
\mu = \mu_t - \sigma \frac{\phi(z)}{1 - \Phi(z)}, 
\qquad 
\sigma^2 = \frac{\sigma_t^2}{1 - \frac{\phi(z)}{1 - \Phi(z)} \left( z - \frac{\phi(z)}{1 - \Phi(z)} \right)}
\]

En resumen, el método propuesto por Cho et al (1989) propone una reconstrucción de las colas faltantes por el truncamiento a la hora de umbralizar. Con esta reconstrucción, el criterio de error de Kittler se evalúa con parámetros más fieles a la realidad, obteniendo así una umbralización más precisa.

% --------------------------------------------------------
% REFERENCIAS
% --------------------------------------------------------
\begin{thebibliography}{99}
\bibitem{cho1989}
Cho, Z. H., Haralick, R. M., \& Yi, S. Y. (1989).
\textit{Improvement of Kittler and Illingworth's minimum error thresholding}.
Pattern Recognition Letters, 9(1), 1–6.
\end{thebibliography}

\end{document}